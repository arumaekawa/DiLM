base:
  experiment_name: train.${generator.model_name}.${learner.model_name}.${data.task_name}
  method: dilm
  run_name: ${base.method}.lm.${base.sub_run_name}
  sub_run_name: step_${train.total_train_step}
  save_dir_root: ./save
  save_method_dir: ${base.save_dir_root}/${base.experiment_name}/${base.method}.lm
  save_dir: ${base.save_method_dir}/${base.sub_run_name}
  data_dir_root: ./data
  seed: 42
  device: null

data:
  task_name: sst2
  datasets_path: ${base.data_dir_root}/${data.task_name}/datasets
  preprocessed_datasets_path: ${base.data_dir_root}/${data.task_name}/datasets_${generator.model_name}_${learner.model_name}
  train_batch_size: 64
  valid_batch_size: 256
  test_batch_size: 256
  num_proc: 1
  force_preprocess: False

coreset:
  coreset_type: k_centers # {random, k_centers, herding, rank_dilm}
  model_name: bert-base-uncased
  save_dir: ${base.data_dir_root}/${data.task_name}/coresets/${coreset.coreset_type}

generator:
  model_name: gpt2
  pretrained_model_dir: null
  checkpoint_name: null
  top_p: 0.95
  top_k: null
  repetition_penalty: 1.0
  generate_batch_size: 512
  generate_max_length: null
  generate_fp16: False
  generate_bf16: True
  gradient_checkpointing: True

learner:
  model_name: bert-base-uncased
  use_pretrained_model: True
  disable_dropout: False
  freeze_bert: False
  gradient_checkpointing: True

train:
  train_type: lm

  gm_syn_dpc: null
  gm_real_dpc: null
  gm_real_grad_accum_step: null

  lm_lambda: null
  lm_batch_size: 64

  repset_teacher: null
  n_repset: null

  classifier_grad_only: False

  normalize_temperature: 1.0
  use_generated_data: True

  total_train_step: 80000
  inner_loop: null
  model_step_per_inner_step: null

  lr: 1.0e-5
  optimizer_type: adamw # [sgd, adam, adamw]
  scheduler_type: cosine
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  val_interval: 5000
  val_skip_step: 1
  log_interval: 100

  save_model_dir: ${base.save_dir}/generator
  save_valid_result_dir: ${base.save_dir}/valid_results
  fp16: False
  bf16: True

distilled_data:
  dpc: 20
  n_dataset: 10
  save_dataset_path: ${base.save_dir}/dataset
  over_sample_ratio: 100.0

evaluate:
  task_name: ${data.task_name}
  n_eval_per_dataset: 5
  fp16: False
  bf16: True
  save_result_dir: ${base.save_dir}/final_results

  # training config
  optimizer_type: adamw # ["sgd", "momentum", "adam", "adamw"]
  scheduler_type: cosine
  lr: 1.0e-4
  max_grad_norm: 1.0
  weight_decay: 0.01
  warmup_ratio: 0.5

  train_step: 200
  batch_size: 64

hydra:
  run:
    dir: ${base.save_dir}
  sweep:
    dir: ${base.save_method_dir}
    subdir: ${base.sub_run_name}
